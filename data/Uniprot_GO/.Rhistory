#devtools::install_github("bcm-uga/lfmm")
library(lfmm)
library(tseries)
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-c(2,2,2,0,0,0,0)
test_pos<-rep(test, each=20)
# get genotype data, columns are positions, rows are individuals
#Y <- example.data$genotype
Y <- test_pos
#principal component analysis (PCA) can reveal some ‘structure’ in the genotypic data.
#We perfom PCA by using the prcomp function as follows.
pc <- prcomp(Y)
plot(pc$sdev[1:20]^2, xlab = 'PC', ylab = "Variance explained")
points(6,pc$sdev[6]^2, type = "h", lwd = 3, col = "blue")
#order of vcf: BOD,CAP,FOG,KIB,LOM,SAN,TER
BOD <- rep(0.05026657, each=20)
CAP <- rep(0.07188418, each=20)
FOG <- rep(0.1780976, each=20)
rest <- rep(c(0.003375068,0.00729927,0,0), each=20)
X<- c(BOD, CAP, FOG, rest)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 7) #determined from PCA
y
Y
X
cbind(test, test2)
test2<-c(1,1,1,3,3,3,3)
cbind(test, test2)
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=10)
test2<-reo(c(1,1,1,3,3,3,3), each=10)
test2<-rep(c(1,1,1,3,3,3,3), each=10)
cbind(test, test2)
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=10)
test2<-rep(c(1,1,1,3,3,3,3), each=10)
test3<-rep(c(1,1,1,1,1,1,1), each=10)
cbind(test, test2, test3)
Y<-cbind(test, test2, test3)
#principal component analysis (PCA) can reveal some ‘structure’ in the genotypic data.
#We perfom PCA by using the prcomp function as follows.
pc <- prcomp(Y)
plot(pc$sdev[1:20]^2, xlab = 'PC', ylab = "Variance explained")
points(6,pc$sdev[6]^2, type = "h", lwd = 3, col = "blue")
#order of vcf: BOD,CAP,FOG,KIB,LOM,SAN,TER
BOD <- rep(0.05026657, each=20)
CAP <- rep(0.07188418, each=20)
FOG <- rep(0.1780976, each=20)
rest <- rep(c(0.003375068,0.00729927,0,0), each=20)
X<- c(BOD, CAP, FOG, rest)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 7) #determined from PCA
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
Y
X
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(1,1,1,1,1,1,1), each=20)
Y<-cbind(test, test2, test3)
Y
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pvalues <- pv$calibrated.pvalue
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(1,1,1,1,1,1,1), each=20)
Y<-cbind(test, test2, test3)
#order of vcf: BOD,CAP,FOG,KIB,LOM,SAN,TER
BOD <- rep(1, each=20)
CAP <- rep(1, each=20)
FOG <- rep(1, each=20)
rest <- rep(c(0,0,0,0), each=20)
X<- c(BOD, CAP, FOG, rest)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pvalues <- pv$calibrated.pvalue
Y
pv
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(1,2,2,0,0,1,1), each=20)
Y<-cbind(test, test2, test3)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pv
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(1,1,2,0,0,1,1), each=20)
Y<-cbind(test, test2, test3)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pv
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(2,1,2,0,0,0,1), each=20)
Y<-cbind(test, test2, test3)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pv
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(2,1,2,0,0,0,0), each=20)
Y<-cbind(test, test2, test3)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pv
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(2,2,2,0,0,0,0), each=20)
Y<-cbind(test, test2, test3)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pv
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(1,2,2,0,0,0,0), each=20)
Y<-cbind(test, test2, test3)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pv
View(Y)
library(tidyverse)
setwd("~/Desktop/urchin_adaptation/data")
# read in LD results file
LdValues <- read_table("resultLD.ld")
average_dist <- 1
# calculate LD in average_dist kb bins to display the trendline
averageLD <- LdValues %>%
mutate(markerDistance = abs(BP_B - BP_A)/1000) %>%
dplyr::filter(markerDistance < 5000) %>%
mutate(intervals = cut_width(markerDistance, average_dist, boundary = 0)) %>%
group_by(intervals) %>%
summarise_at(vars(R2),funs(mean(., na.rm=TRUE))) %>%
rename(averageR2=R2)
# calculate inter marker distances
fullLD <- LdValues %>%
mutate(markerDistance = abs(BP_B - BP_A)/1000) %>%
dplyr::filter(markerDistance < 5000) %>%
mutate(intervals = cut_width(markerDistance, average_dist, boundary = 0))
#merge the two data sets (full LD info and average per bin)
mergedLD <- full_join(fullLD,averageLD, by = "intervals")
# visualize LD decay
ggplot(mergedLD) +
geom_point(aes(x=markerDistance, y=R2)) +
geom_line(aes(x=markerDistance, y=averageR2), color="red", size=2) +
geom_vline(xintercept = 2.5) + xlim(0, 100)
# visualize LD decay
ggplot(mergedLD) +
geom_point(aes(x=markerDistance, y=R2)) +
geom_line(aes(x=markerDistance, y=averageR2), color="red", size=2) +
geom_vline(xintercept = 2) + xlim(0, 100)
# visualize LD decay
ggplot(mergedLD) +
geom_point(aes(x=markerDistance, y=R2)) +
geom_line(aes(x=markerDistance, y=averageR2), color="red", size=2) +
geom_vline(xintercept = 1.5) + xlim(0, 100)
library(topGO)
setwd("~/Desktop/urchin_adaptation/data/Uniprot_GO")
geneID2GO <- readMappings("GO_mapping_topGO") # uniprot to GO mapping
geneNames <- names(geneID2GO)
dotopgo <- function(myfile, gokind, outname){
myfilename<-paste("temp_files_for_GO/", myfile, sep="")
print(myfilename)
myInterestingGenes <- read.csv(myfilename, header = FALSE) # list of interesting genes, output of LOC to uniprot mapping
intgenes <- myInterestingGenes[, "V1"]
geneList <- factor(as.integer(geneNames %in% intgenes)) # mask of 0 and 1 if geneName is interesting
names(geneList) <- geneNames # geneList but annotated with the gene names
GOdata <- new("topGOdata",
ontology = gokind, # ontology of interest (BP, MF or CC)
allGenes = geneList,
annot = annFUN.gene2GO,
gene2GO = geneID2GO)
resultFisher <- runTest(GOdata, algorithm = "classic", statistic = "fisher") # these are the options I'll be using! checked!
allRes <- GenTable(GOdata, classicFisher = resultFisher, topNodes = sum(resultFisher@score < 0.01)) # top 10 enriched terms
write.csv(allRes,outname, row.names = FALSE)
}
dotopgo("uniprotIDs_pcangsd_all.txt", "BP", "all_pcangsd_BP.csv")
dotopgo("uniprotIDs_pcangsd_all.txt", "MF", "all_pcangsd_MF.csv")
