#devtools::install_github("bcm-uga/lfmm")
library(lfmm)
library(tseries)
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-c(2,2,2,0,0,0,0)
test_pos<-rep(test, each=20)
# get genotype data, columns are positions, rows are individuals
#Y <- example.data$genotype
Y <- test_pos
#principal component analysis (PCA) can reveal some ‘structure’ in the genotypic data.
#We perfom PCA by using the prcomp function as follows.
pc <- prcomp(Y)
plot(pc$sdev[1:20]^2, xlab = 'PC', ylab = "Variance explained")
points(6,pc$sdev[6]^2, type = "h", lwd = 3, col = "blue")
#order of vcf: BOD,CAP,FOG,KIB,LOM,SAN,TER
BOD <- rep(0.05026657, each=20)
CAP <- rep(0.07188418, each=20)
FOG <- rep(0.1780976, each=20)
rest <- rep(c(0.003375068,0.00729927,0,0), each=20)
X<- c(BOD, CAP, FOG, rest)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 7) #determined from PCA
y
Y
X
cbind(test, test2)
test2<-c(1,1,1,3,3,3,3)
cbind(test, test2)
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=10)
test2<-reo(c(1,1,1,3,3,3,3), each=10)
test2<-rep(c(1,1,1,3,3,3,3), each=10)
cbind(test, test2)
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=10)
test2<-rep(c(1,1,1,3,3,3,3), each=10)
test3<-rep(c(1,1,1,1,1,1,1), each=10)
cbind(test, test2, test3)
Y<-cbind(test, test2, test3)
#principal component analysis (PCA) can reveal some ‘structure’ in the genotypic data.
#We perfom PCA by using the prcomp function as follows.
pc <- prcomp(Y)
plot(pc$sdev[1:20]^2, xlab = 'PC', ylab = "Variance explained")
points(6,pc$sdev[6]^2, type = "h", lwd = 3, col = "blue")
#order of vcf: BOD,CAP,FOG,KIB,LOM,SAN,TER
BOD <- rep(0.05026657, each=20)
CAP <- rep(0.07188418, each=20)
FOG <- rep(0.1780976, each=20)
rest <- rep(c(0.003375068,0.00729927,0,0), each=20)
X<- c(BOD, CAP, FOG, rest)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 7) #determined from PCA
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
Y
X
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(1,1,1,1,1,1,1), each=20)
Y<-cbind(test, test2, test3)
Y
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pvalues <- pv$calibrated.pvalue
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(1,1,1,1,1,1,1), each=20)
Y<-cbind(test, test2, test3)
#order of vcf: BOD,CAP,FOG,KIB,LOM,SAN,TER
BOD <- rep(1, each=20)
CAP <- rep(1, each=20)
FOG <- rep(1, each=20)
rest <- rep(c(0,0,0,0), each=20)
X<- c(BOD, CAP, FOG, rest)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pvalues <- pv$calibrated.pvalue
Y
pv
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(1,2,2,0,0,1,1), each=20)
Y<-cbind(test, test2, test3)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pv
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(1,1,2,0,0,1,1), each=20)
Y<-cbind(test, test2, test3)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pv
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(2,1,2,0,0,0,1), each=20)
Y<-cbind(test, test2, test3)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pv
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(2,1,2,0,0,0,0), each=20)
Y<-cbind(test, test2, test3)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pv
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(2,2,2,0,0,0,0), each=20)
Y<-cbind(test, test2, test3)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pv
#BOD,CAP,FOG,KIB,LOM,SAN,TER
test<-rep(c(2,2,2,0,0,0,0), each=20)
test2<-rep(c(1,1,1,3,3,3,3), each=20)
test3<-rep(c(1,2,2,0,0,0,0), each=20)
Y<-cbind(test, test2, test3)
## Fit an LFMM, i.e, compute B, U, V estimates
# ridge vs lasso:
# ridge minimizes sum of squared residuals + lambda * slope squared -> less sensitive to changes in x, lambda is a scaler, the larger lambda the less sensitive y is to x (i.e. flatter the fitted line)
# overall shrinks parameters, making our predictions less sensitive to them
# vs lasso regression: sum of squared residuals + lambda * absolute_value(slope), again making prediction (y) less sensitive to x
# lasso can shrink parameters to 0, ridge can't (when increasing lambda) -> lasso can exclude useless variables from our model
# ridge is more useful when most parameters are useful -> are most of my parameters useful? I think so!
# so I need ridge... let's do both tho and compare results?
# ridge is also the one used in most papers citing the lfmm paper
mod.lfmm <- lfmm_ridge(Y = Y,
X = X,
K = 2) #determined from PCA
## performs association testing using the fitted model:
pv <- lfmm_test(Y = Y,
X = X,
lfmm = mod.lfmm,
calibrate = "gif") #gif is default, and what others use
pv
View(Y)
